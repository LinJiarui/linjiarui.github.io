---
title: "ARCBERT: 面向土木建筑领域的大规模语料库及领域预训练模型"
lang: zh
ref: portfolio/2022-04-02-ARCBERT-largescale-dataset-and-pretrained-model-for-AEC-domain
permalink: /portfolio/2022-04-02-ARCBERT-largescale-dataset-and-pretrained-model-for-AEC-domain
excerpt: "课题组面向土木建筑领域NLP应用，构建了首个大规模领域语料库和领域预训练模型，有关模型在分类、命名实体识别等 多个任务中性能表现显著优于传统方法，F1最大提升分别为可达3.8%和8.1%"
collection: portfolio
date: 2022-04-02

category: data
tags:
  - data
  - source code
  - pretrained language model
  - domain specific model
  - large foundation model
  - domain corpora
  - deep learning
  - NLP
  - research
---

自2019年以来，课题组专注于如何让计算机高效理解规范、施工组织文档等复杂的领域文本，并从中提取复杂领域知识。为实现大规模领域语料中复杂领域先验知识的高效利用与迁移，课题组构建了首个面向土木建筑领域的大规模领域语料库和领域预训练模型，并在分类、命名实体识别等 多个任务中实现性能的大幅提升，F1值相较传统方法最大提升分别可达3.8%和8.1%。有关数据集、预训练模型及算法可[在此]({{site.baseurl}}/files/2022-04-02-ARCBERT-largescale-dataset-and-pretrained-model-for-AEC-domain.zip)下载，供大家研究参考。最新算法、数据集及预训练模型更新可在[github主页](https://github.com/smartaec/AEC-domain-corpora)跟踪关注。同时欢迎关注课题组微信公众号（智能土木ABC，点击左侧链接查看二维码）了解更多有趣的研究。

当您使用或参考以上成果时，请引用以下关键研究论文：

[Zheng, Z., Lu, X.Z., Chen, K.Y., Zhou, Y.C., Lin, J.R. (2022). Pretrained Domain-Specific Language Model for Natural Language Processing Tasks in the AEC Domain. <i>Computers in Industry</i>, 142, 103733.]({{site.baseurl}}/publications/2022-06-13-pretrained-domain-specific-language-model-for-NLP-tasks-in-AEC)
