---
title: "ARCBERT: Largescale Domain Specific Dataset and Pretrained Language Model for AEC Industry"
lang: en
ref: portfolio/2022-04-02-ARCBERT-largescale-dataset-and-pretrained-model-for-AEC-domain
permalink: /en/portfolio/2022-04-02-ARCBERT-largescale-dataset-and-pretrained-model-for-AEC-domain
excerpt: "We developed and opensource the first largescale domain specific dataset and pretrained language model for NLP applications in the AEC industry, which outperformed traditional methods in various NLP tasks"
collection: portfolio
date: 2022-04-02

category: data
tags:
  - data
  - source code
  - pretrained language model
  - domain specific model
  - large foundation model
  - domain corpora
  - deep learning
  - NLP
  - research
---

Sine early 2019, our group devotes most of our efforts on developing new methods to extract and learn complex domain knowledge from textual documents such as building codes, construction documents. To efficiently extract and transfer prior knowledge hidden in domain documents, we developed the first largescale domain specific corpora and pretrained language model based on BERT, which outperformed traditional methods in various NLP tasks with maximum improvement of 8.1% You can download the dataset, pretrained models and algorithms [here]({{site.baseurl}}/files/2022-04-02-ARCBERT-largescale-dataset-and-pretrained-model-for-AEC-domain.zip) for research and exploration purpose. Latest updates of the dataset, pretrained models and algorithms could be found at [github page](https://github.com/smartaec/AEC-domain-corpora).


If our work is adopted or used in your work, please cite the following articlesï¼š

[Zheng, Z., Lu, X.Z., Chen, K.Y., Zhou, Y.C., Lin, J.R.* (2022). Pretrained Domain-Specific Language Model for Natural Language Processing Tasks in the AEC Domain. <i>Computers in Industry</i>, 142, 103733.]({{site.baseurl}}/en/publications/2022-06-13-pretrained-domain-specific-language-model-for-NLP-tasks-in-AEC)
